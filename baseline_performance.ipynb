{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multitask Baseline Performance\n",
    "\n",
    "In this notebook I will obtain the baseline performances obtainable using simple machine learning on the \"Cookie Theft Picture Test\" section of the DementiaBank dataset. <br />\n",
    "The numerical features are extracted using my feature extraction project [Link here](https://github.com/EdoStoppa/Dementia_Features_Extractor). Furthermore I will define some baseline performance dropping one feature category at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General import useful for each part of the notebook\n",
    "import os\n",
    "import numpy as np\n",
    "import statistics\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "from typing import Tuple, Callable, Any\n",
    "\n",
    "# Import used to manipulate all the dataset for training/testing and hyperparameters search\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "\n",
    "# Regression Models\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, BaggingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Regression Models Evaluation\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Classification Models (both Binary and Multi Class)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, BaggingClassifier\n",
    "\n",
    "# Classification Models Evaluation\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Needed to suppress some convergence warnings\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "# Needed to force white background to all plots\n",
    "%config InlineBackend.print_figure_kwargs={'facecolor' : \"w\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary Work\n",
    "This section contains some fuction and variable definition that are useful in the Notebook.<br />\n",
    "Please, check all the paths and flags before executing the entire notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining some useful variables\n",
    "\n",
    "# Set the project folder\n",
    "PRJ_FOLDER = '.'\n",
    "# Set the folder the full dataset is located\n",
    "DATA_FOLDER = os.path.join(PRJ_FOLDER, 'data')\n",
    "# Set the name of the file that holds the full dataset\n",
    "FULL_DATASET_NAME = 'feature_dataset.csv'\n",
    "# Set the name of the file that holds the angraphic information\n",
    "ANAGRAPHIC_DATASET_NAME = 'anagraphic_info.csv'\n",
    "# Create a list with all the dataset names excluding the full dataset\n",
    "SEP_DATASETS_FOLDER = os.path.join(DATA_FOLDER, 'separate_datasets')\n",
    "# Set the folder where all the results will be saved\n",
    "RES_FOLDER = os.path.join(PRJ_FOLDER, 'results')\n",
    "# Flag needed to explicit if partial datasets should be built\n",
    "BUILD_PARTIAL_DATASET = True\n",
    "# Flag needed to explicit if hyperparameters random search must be performed\n",
    "# If false the notebook search a previously saved pickle file for the parameters\n",
    "HYPER_SEARCH = True\n",
    "\n",
    "print(f'Folder where the entire project is located: {PRJ_FOLDER}')\n",
    "print(f'Folder where the full dataset is saved: {DATA_FOLDER}')\n",
    "print(f'Name of the full dataset: {FULL_DATASET_NAME}')\n",
    "print(f'Name of the anagraphic dataset: {ANAGRAPHIC_DATASET_NAME}')\n",
    "print(f'Separate features group datasets folder: {SEP_DATASETS_FOLDER}')\n",
    "print(f'Folder where all results will be saved: {RES_FOLDER}')\n",
    "print(f'Flag for building partial datasets set to {BUILD_PARTIAL_DATASET}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets Loading\n",
    "In this section there are two functions used to load all the required data, and another function that is used to craft all the partial datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the flag is TRUE, create all the partial datasets and save them in DATA_FOLDER\n",
    "\n",
    "if BUILD_PARTIAL_DATASET:\n",
    "    # Load the full dataset into pandas\n",
    "    full_dataset = pd.read_csv(os.path.join(DATA_FOLDER, FULL_DATASET_NAME))\n",
    "\n",
    "    for to_remove in os.listdir(SEP_DATASETS_FOLDER):\n",
    "        # Avoid removing the anagraphic information\n",
    "        if to_remove != ANAGRAPHIC_DATASET_NAME:\n",
    "            # Get all the names of the features that should be removed\n",
    "            to_remove_dataframe = pd.read_csv(os.path.join(SEP_DATASETS_FOLDER, to_remove))\n",
    "            to_remove_columns = to_remove_dataframe.columns.to_list()[1:]\n",
    "\n",
    "            # Create a partial dataset dataframe\n",
    "            new_dataset = full_dataset.drop(to_remove_columns, axis=1)\n",
    "\n",
    "            # Save the dataset\n",
    "            with open(os.path.join(DATA_FOLDER, f'{to_remove.split(\".\")[0]}_removed.csv'), 'w+') as out:\n",
    "                new_dataset.to_csv(out, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a simple function that loads a single dataset\n",
    "\n",
    "def load_raw_data(csv_path: str, feat_to_predict: str) -> Tuple[np.ndarray, np.ndarray, list]:\n",
    "    # Load the entire csv file\n",
    "    data = pd.read_csv(csv_path)\n",
    "\n",
    "    # Get only the labels\n",
    "    labels = pd.DataFrame(data[feat_to_predict], columns=[feat_to_predict])\n",
    "\n",
    "    # Remove the unnecessary columns\n",
    "    data = data.drop(['id', 'mmse', 'bin_class', 'multi_class3', 'multi_class4', 'multi_class5'], axis=1)\n",
    "\n",
    "    return data.to_numpy(), labels.to_numpy().reshape((len(labels), )), data.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function loads all datasets, and make them ready for training/testing\n",
    "\n",
    "def load_processed_data(sep_data_folder: str, data_folder: str, feat_to_predict: str, num_folds: int):\n",
    "    DATASETS = []\n",
    "    FOLDS = []\n",
    "    COLUMNS = []\n",
    "\n",
    "    # Get the name of the folder containing the partial datasets\n",
    "    name_sep_folder = sep_data_folder.split(os.path.sep)[-1]\n",
    "    # Get the name of all datasets present\n",
    "    DATASETS_NAMES = [dataset for dataset in os.listdir(data_folder) if dataset != name_sep_folder]\n",
    "\n",
    "    # Load every dataset \n",
    "    for name in DATASETS_NAMES:\n",
    "        print(f'Loading {name}')\n",
    "        # Get the raw data\n",
    "        data, labels, cols = load_raw_data(os.path.join(data_folder, name), feat_to_predict)\n",
    "        # Create the iterator responsible of dividing the dataset to do KFold cross validation\n",
    "        kfold = StratifiedKFold(n_splits=num_folds)\n",
    "        # Divide the data in training and test set\n",
    "        folds = []\n",
    "        for train_index, test_index in kfold.split(data, labels):\n",
    "            folds.append((train_index, test_index))\n",
    "        \n",
    "        DATASETS.append((data, labels))\n",
    "        FOLDS.append(folds)\n",
    "        COLUMNS.append(cols)\n",
    "    \n",
    "    print('\\nAll data loaded!')\n",
    "    return DATASETS_NAMES, DATASETS, COLUMNS, FOLDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_grid_search(X, y, models: list, spaces: list, scoring: str, num_jobs=-1) -> list:\n",
    "    # Get the cross validator\n",
    "    cv = StratifiedKFold(n_splits=5)\n",
    "\n",
    "    # Perform the Hyperparameter Grid Search\n",
    "    final_parameters = {}\n",
    "    for (model, name), space in zip(models, spaces):\n",
    "        print(f'Searching {name} hyperparameters space...')\n",
    "        # Create the GridSearch object\n",
    "        search = GridSearchCV(model, space, scoring=scoring, n_jobs=num_jobs, cv=cv)\n",
    "        # Perform the seach\n",
    "        search = search.fit(X, y)\n",
    "        # Save and display the results\n",
    "        final_parameters[name] = search.best_params_\n",
    "        print('Best parameters: ', search.best_params_, '\\n')\n",
    "\n",
    "    print('\\nEverything complete!')\n",
    "    return final_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to train both the regression and classification models\n",
    "\n",
    "def train_models(models_generator: Callable[[], Any], datasets: list, datasets_names: list, kfolds: list, model_params: list) -> list:\n",
    "    ready_models = []\n",
    "    for data, data_name, folds in zip(datasets, datasets_names, kfolds):\n",
    "        print(f'Working on {data_name}')\n",
    "\n",
    "        # Unwrap data\n",
    "        dataset, label = data\n",
    "        \n",
    "        # Train a model for each fold\n",
    "        trained_folds = []\n",
    "        for fold in folds:\n",
    "            train_index, _ = fold\n",
    "            X_train, y_train = dataset[train_index], label[train_index]\n",
    "\n",
    "            # Generate new batch of models\n",
    "            models = models_generator()\n",
    "            \n",
    "            fold_models = []\n",
    "            for model, model_name in models:\n",
    "                # Set the parameters of the model\n",
    "                model.set_params(**model_params[model_name])\n",
    "                # Train model\n",
    "                model.fit(X_train, y_train)\n",
    "                # Save model\n",
    "                fold_models.append((model_name, model))\n",
    "\n",
    "            # Save all trained models for the fold\n",
    "            trained_folds.append(fold_models)\n",
    "\n",
    "        # Save all the trained fold models for the daaset\n",
    "        ready_models.append(trained_folds)\n",
    "\n",
    "    print('\\nEverything complete!')\n",
    "    return ready_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(lbls: list, avgs: dict, stdevs: dict, y_name: str, title: str, dimensions: tuple, precision=1.0):\n",
    "    # Fix figure dimensions\n",
    "    plt.rcParams[\"figure.figsize\"] = dimensions\n",
    "\n",
    "    # Extract the max value on the Y axis\n",
    "    max_avg_y = max([max(item) for _, item in avgs.items()])\n",
    "    max_stdev_y = max([max(item) for _, item in stdevs.items()])\n",
    "    max_y = np.ceil(max_avg_y) if max_avg_y + max_stdev_y < np.ceil(max_avg_y) else np.ceil(max_avg_y + max_stdev_y)\n",
    "\n",
    "    # Format the data in order to be fed to Pandas\n",
    "    algos = [key for key, _ in avgs.items()]\n",
    "\n",
    "    avgs = [item for _, item in avgs.items()]\n",
    "    avgs = list(map(list, zip(*avgs)))\n",
    "\n",
    "    stdevs = [item for _, item in stdevs.items()]\n",
    "    stdevs = list(map(list, zip(*stdevs)))\n",
    "\n",
    "    # Create positions of the bars\n",
    "    w = 0.12\n",
    "    xticks = [np.arange(len(avgs[0]))]\n",
    "    for _ in range(1, len(avgs)):\n",
    "        xticks.append(xticks[-1] + w)\n",
    "\n",
    "    # Actually plot everything\n",
    "    for values, stdev, pos, lbl in zip(avgs, stdevs, xticks, lbls):\n",
    "        plt.bar(pos, values, w, label=lbl)\n",
    "        plt.errorbar(pos, values, yerr=stdev, fmt=\"o\", ecolor='black', capsize=3, color='black')\n",
    "\n",
    "    # Choose the right position to put the labels on\n",
    "    num_datasets = len(avgs)\n",
    "    if num_datasets % 2 == 0:\n",
    "        idx = num_datasets//2 - 1\n",
    "        pos = xticks[idx] + w/2\n",
    "    else:\n",
    "        idx = (num_datasets - 1) // 2\n",
    "        pos = xticks[idx]\n",
    "\n",
    "    # Fix tile and y/x labels\n",
    "    SMALL_SIZE = 12\n",
    "    MEDIUM_SIZE = 14\n",
    "    BIGGER_SIZE = 20\n",
    "    plt.title(title, fontsize=BIGGER_SIZE)\n",
    "    plt.ylabel(y_name, fontsize=MEDIUM_SIZE)\n",
    "    plt.xlabel('Model', fontsize=MEDIUM_SIZE)\n",
    "    plt.xticks(pos, algos, rotation = 30, fontsize=SMALL_SIZE)\n",
    "    plt.yticks(np.arange(0, max_y, precision), fontsize=SMALL_SIZE)\n",
    "    # Finish everything and show it\n",
    "    plt.legend(fontsize=SMALL_SIZE)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to evaluate the regression models\n",
    "\n",
    "def evaluate_regression_models(ready_models: list, datasets: list, datasets_names: list, kfolds: list) -> list:\n",
    "    evaluation_results = []\n",
    "    avg_to_plot = {name[0]: [] for name in ready_models[0][0]}\n",
    "    std_to_plot = {name[0]: [] for name in ready_models[0][0]}\n",
    "    \n",
    "    for kmodels_matrix, folds, data, name in zip(ready_models, kfolds, datasets, datasets_names):\n",
    "        # Preprocess the name removing the file extension\n",
    "        name = name.split('.')[0]\n",
    "        # Unwrap data\n",
    "        dataset, label = data\n",
    "        \n",
    "        # Reorganize the models\n",
    "        kmodels_matrix = list(map(list, zip(*kmodels_matrix)))\n",
    "        out = ''\n",
    "        for models_list in kmodels_matrix:            \n",
    "            mae, rmse, r2 = [], [], []\n",
    "            for (model_name, model), fold in zip(models_list, folds):\n",
    "                _, test_index = fold\n",
    "                X_test, y_test = dataset[test_index], label[test_index]\n",
    "                # Compute the predictions for the test dataset\n",
    "                predictions = model.predict(X_test)\n",
    "\n",
    "                mae.append(mean_absolute_error(y_test, predictions))\n",
    "                rmse.append(np.sqrt(mean_squared_error(y_test, predictions)))\n",
    "                r2.append(r2_score(y_test, predictions))\n",
    "            \n",
    "            # Compute the average and standard deviation of each metric\n",
    "            mae_avg, mae_std = statistics.mean(mae), statistics.stdev(mae)\n",
    "            rmse_avg, rmse_std = statistics.mean(rmse), statistics.stdev(rmse)\n",
    "            r2_avg, r2_std = statistics.mean(r2), statistics.stdev(r2)\n",
    "\n",
    "            # Save data in dictionary ready to be plotted\n",
    "            avg_to_plot[model_name].append(rmse_avg)\n",
    "            std_to_plot[model_name].append(rmse_std)\n",
    "\n",
    "            # Compute all the interesting metrics\n",
    "            out += f'{model_name}\\n\\n'\n",
    "            out += f'MAE                \\tAvg: {mae_avg:.3f}\\tStdDev: {mae_std:.3f}\\n'\n",
    "            out += f'RMSE               \\tAvg: {rmse_avg:.3f}\\tStdDev: {rmse_std:.3f}\\n'\n",
    "            out += f'R2                 \\tAvg: {r2_avg:.3f}\\tStdDev: {r2_std:.3f}\\n'\n",
    "            out += '\\n****************************************************************\\n\\n'\n",
    "\n",
    "        evaluation_results.append((name, out))\n",
    "\n",
    "    plot_results(datasets_names, avg_to_plot, std_to_plot, 'RMSE', 'Regression Performance', (15,9), precision=0.5)\n",
    "    \n",
    "    return evaluation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fucntion that compute the metrics for the binary classification task\n",
    "def compute_metrics_bin(true: np.ndarray, pred: np.ndarray, lbls: list):\n",
    "    acc  = accuracy_score(true, pred)\n",
    "    prec = precision_score(true, pred)\n",
    "    rec  = recall_score(true, pred)\n",
    "    f1   = f1_score(true, pred)\n",
    "\n",
    "    return acc, prec, rec, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fucntion that compute the metrics for the multiclass classification task\n",
    "def compute_metrics_multi(true: np.ndarray, pred: np.ndarray, lbls: list):\n",
    "    # Generate a list of int representing all the available classes\n",
    "    classes = np.arange(0, len(np.bincount(lbls)), 1)\n",
    "    # Compute the confusion matrix\n",
    "    mtx = confusion_matrix(true, pred, labels=classes\n",
    "    )\n",
    "    # Compute the total accuracy\n",
    "    acc = sum([mtx[i, i] for i in classes]) / np.sum(mtx)\n",
    "\n",
    "    # Compute the list of precision, recall, and f1 score for each class\n",
    "    precs, recs, f1s = [], [], [] \n",
    "    for idx in classes:\n",
    "        sum_prec = np.sum(mtx[:, idx])\n",
    "        precs.append(mtx[idx, idx] / sum_prec if sum_prec > 0 else 0)\n",
    "\n",
    "        sum_rec = np.sum(mtx[idx, :])\n",
    "        recs.append(mtx[idx, idx] / sum_rec if sum_rec > 0 else 0)\n",
    "        \n",
    "        nan_flag = precs[-1]==0 and recs[-1]==0\n",
    "        new_f1 = 0 if nan_flag else 2 * (precs[-1] * recs[-1]) / (precs[-1] + recs[-1])\n",
    "        f1s.append(new_f1)\n",
    "\n",
    "    # Extract the final score (uin the macro-average)\n",
    "    tot_prec = np.average(precs)\n",
    "    tot_rec = np.average(recs)\n",
    "    tot_f1 = np.average(f1s)\n",
    "\n",
    "    return acc, tot_prec, tot_rec, tot_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to evaluate the classification models\n",
    "\n",
    "def evaluate_classification_models(ready_models: list, datasets: list, datasets_names: list, kfolds: list, typ: str, compute_metr) -> list:\n",
    "    evaluation_results = []\n",
    "    avg_to_plot = {name[0]: [] for name in ready_models[0][0]}\n",
    "    std_to_plot = {name[0]: [] for name in ready_models[0][0]}\n",
    "    \n",
    "    for kmodels_matrix, folds, data, name in zip(ready_models, kfolds, datasets, datasets_names):\n",
    "        # Preprocess the name removing the file extension\n",
    "        name = name.split('.')[0]\n",
    "        # Unwrap data\n",
    "        dataset, label = data\n",
    "        \n",
    "        # Reorganize the models\n",
    "        kmodels_matrix = list(map(list, zip(*kmodels_matrix)))\n",
    "        out = ''\n",
    "        for models_list in kmodels_matrix:\n",
    "                       \n",
    "            acc, prec, rec, f1 = [], [], [], []\n",
    "            for (model_name, model), fold in zip(models_list, folds):\n",
    "                _, test_index = fold\n",
    "                X_test, y_test = dataset[test_index], label[test_index]\n",
    "                # Compute the predictions for the test dataset\n",
    "                predictions = model.predict(X_test)\n",
    "\n",
    "                metrics = compute_metr(y_test, predictions, label)\n",
    "                acc.append(metrics[0])\n",
    "                prec.append(metrics[1])\n",
    "                rec.append(metrics[2])\n",
    "                f1.append(metrics[3])\n",
    "                \n",
    "            # Compute the average and standard deviation of each metric\n",
    "            acc_avg, acc_std = statistics.mean(acc), statistics.stdev(acc)\n",
    "            prec_avg, prec_std = statistics.mean(prec), statistics.stdev(prec)\n",
    "            rec_avg, rec_std = statistics.mean(rec), statistics.stdev(rec)\n",
    "            f1_avg, f1_std = statistics.mean(f1), statistics.stdev(f1)\n",
    "\n",
    "            # Save data in dictionary ready to be plotted\n",
    "            avg_to_plot[model_name].append(acc_avg)\n",
    "            std_to_plot[model_name].append(acc_std)\n",
    "\n",
    "            # Compute all the interesting metrics\n",
    "            \n",
    "            out += f'{model_name}\\n\\n'\n",
    "            out += f'Accuracy:          \\tAvg: {acc_avg:.3f}\\tStdDev: {acc_std:.3f}\\n'\n",
    "            out += f'Precision:         \\tAvg: {prec_avg:.3f}\\tStdDev: {prec_std:.3f}\\n'\n",
    "            out += f'Recall:            \\tAvg: {rec_avg:.3f}\\tStdDev: {rec_std:.3f}\\n'\n",
    "            out += f'F1:                \\tAvg: {f1_avg:.3f}\\tStdDev: {f1_std:.3f}\\n'\n",
    "            out += '\\n**********************************************\\n\\n'\n",
    "\n",
    "        evaluation_results.append((name, out))\n",
    "\n",
    "    plot_results(datasets_names, avg_to_plot, std_to_plot, 'Accuracy', f'{typ} Classification Performance', (15,9), precision=0.1)\n",
    "    \n",
    "    return evaluation_results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to save the results in the correct files\n",
    "\n",
    "def save_results(results: list, save_folder: str, task: str):\n",
    "    for name, out in results:\n",
    "        with open(os.path.join(save_folder, task, name + '.txt'), 'w+') as file:\n",
    "            file.write(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "In this section we are going to evalute the performance of different regression models.<br />\n",
    "In particular, we're going to predict the MMSE score of each conversation using these models:\n",
    "- Ridge Regression\n",
    "- KNN Regressor\n",
    "- Boosted Trees\n",
    "- Bagged Trees\n",
    "- Decision Tree\n",
    "- Support Vector Machine Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set some important variables first\n",
    "\n",
    "FEATURE_TO_PREDICT = 'mmse'\n",
    "NUM_FOLDS = 5\n",
    "TYPE = 'regression'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple function that generates a fresh batch of regression models ready to be trained\n",
    "\n",
    "def get_regression_models() -> list:\n",
    "    models = [(Ridge(), 'Ridge Regression'), (KNeighborsRegressor(), 'KNN Regressor'),\n",
    "              (GradientBoostingRegressor(), 'Boosted Trees'), (BaggingRegressor(), 'Bagged Trees'), \n",
    "              (DecisionTreeRegressor(), 'Decision Tree'), (LinearSVR(), 'SVM')]\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple function that returns a list of dictionaries containing the hyperparameters state space of each model\n",
    "\n",
    "def get_regression_hyper_space():\n",
    "    search_spaces = [\n",
    "        # Ridge\n",
    "        {'alpha': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'fit_intercept': [True, False],\n",
    "        'normalize': [True, False],\n",
    "        'max_iter': [1000, 5000, 10000],\n",
    "        'solver': ['svd', 'cholesky', 'lsqr', 'lbfgs']},\n",
    "        # K-Neighbor\n",
    "        {'n_neighbors': [3, 5, 7, 9],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'algorithm': ['ball_tree', 'kd_tree', 'brute'],\n",
    "        'p': [1, 2]},\n",
    "        # Gradient Boosting\n",
    "        {'loss': ['squared_error', 'absolute_error', 'quantile'],\n",
    "        'learning_rate': [0.001, 0.01, 0.1, 1],\n",
    "        'n_estimators': [10, 50, 100, 200],\n",
    "        'criterion': ['friedman_mse', 'squared_error', 'mae'],\n",
    "        'max_depth': [1, 3, 5, 7],\n",
    "        'max_features': ['auto', 'sqrt', 'log2']},\n",
    "        # Bagging\n",
    "        {'n_estimators': [10, 50, 100, 200],\n",
    "        'max_features': [1, 4, 8, 16, 32],\n",
    "        'bootstrap': [True, False],\n",
    "        'bootstrap_features': [True, False],\n",
    "        'oob_score': [True, False],},\n",
    "        # Decision trees\n",
    "        {'criterion': ['friedman_mse', 'squared_error', 'absolute_error', 'poisson'],\n",
    "        'max_depth': [1, 3, 5, 7, None],\n",
    "        'splitter': ['best', 'random'],\n",
    "        'max_features': ['auto', 'sqrt', 'log2'],\n",
    "        'max_leaf_nodes': [None, 2, 4, 8, 16]},\n",
    "        # SVM\n",
    "        {'tol': [1e-5, 1e-4, 1e-3, 1e-2],\n",
    "        'C': [1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100],\n",
    "        'loss': ['epsilon_insensitive', 'squared_epsilon_insensitive'],\n",
    "        'fit_intercept': [True, False],\n",
    "        'max_iter': [1000, 5000, 10000]}\n",
    "    ]\n",
    "\n",
    "    return search_spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the hyperparameters for each model\n",
    "\n",
    "if HYPER_SEARCH:\n",
    "    # Get the full dataset\n",
    "    X, y, cols = load_raw_data(os.path.join(DATA_FOLDER, FULL_DATASET_NAME), FEATURE_TO_PREDICT)\n",
    "    # Perform the search\n",
    "    PARAMS = hyper_grid_search(X, y, get_regression_models(), get_regression_hyper_space(), 'neg_root_mean_squared_error')\n",
    "    # Save the parameters\n",
    "    with open(os.path.join(RES_FOLDER, TYPE, 'params.pickle'), 'wb') as f:\n",
    "        pickle.dump(PARAMS, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "else:\n",
    "    with open(os.path.join(RES_FOLDER, TYPE, 'params.pickle'), 'rb') as f:\n",
    "        PARAMS = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the datasets and preparing them for training & testing\n",
    "DATASETS_NAMES, DATASETS, COLUMNS, FOLDS = load_processed_data(SEP_DATASETS_FOLDER, DATA_FOLDER, FEATURE_TO_PREDICT, NUM_FOLDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all these regression models\n",
    "trained_regression_models = train_models(get_regression_models, DATASETS, DATASETS_NAMES, FOLDS, PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all the models\n",
    "regression_results = evaluate_regression_models(trained_regression_models, DATASETS, DATASETS_NAMES, FOLDS)\n",
    "\n",
    "# Save the results to different files\n",
    "save_results(regression_results, RES_FOLDER, TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification\n",
    "In this section we are going to evaluate multiple classification models.<br />\n",
    "In particular, we are going to predict if each conversation was made by a patient with Dementia or by a control patient. We will use these models:\n",
    "- Decision Tree\n",
    "- Linear Discriminant Classifier\n",
    "- Logistic Regression\n",
    "- Gaussian Naive Bayes\n",
    "- Linear Support Vector Classifier\n",
    "- K-Nearest Neighbors Classifier\n",
    "- Boosted Trees\n",
    "- Bagged Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set some important variables first\n",
    "\n",
    "FEATURE_TO_PREDICT = 'bin_class'\n",
    "NUM_FOLDS = 5\n",
    "TYPE = 'binary_classification'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple function that generates a fresh batch of binary classification models ready to be trained\n",
    "\n",
    "def get_binary_class_models() -> list:\n",
    "    models = [(LogisticRegression(), 'Logistic Regression'), (GaussianNB(), 'Gaussian Naive Bayes'),\n",
    "              (LinearSVC(), 'Linear Support Vector Classifier'), (KNeighborsClassifier(), 'KNN Classifier'),\n",
    "              (GradientBoostingClassifier(), 'Boosted Trees'), (BaggingClassifier(), 'Bagged Trees')]\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple function that returns a list of dictionaries containing the hyperparameters state space of each model\n",
    "\n",
    "def get_bin_hyper_space():\n",
    "    search_spaces = [\n",
    "        # Logistic Regression\n",
    "        {'penalty': ['l1', 'l2', 'none', 'elasticnet'],\n",
    "        'tol': [1e-5, 1e-4, 1e-3, 1e-2],\n",
    "        'C': [1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100],\n",
    "        'fit_intercept': [True, False],\n",
    "        'max_iter': [100, 250, 500],\n",
    "        'solver': ['newton-cg', 'saga', 'sag', 'liblinear', 'lbfgs']},\n",
    "        # Gaussian Naive Bayes\n",
    "        {'var_smoothing': [1e-10, 1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3]},\n",
    "        # SVM\n",
    "        {'penalty': ['l1', 'l2'],\n",
    "        'tol': [1e-6, 1e-5, 1e-4, 1e-3, 1e-4, 1e-3],\n",
    "        'C': [1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100],\n",
    "        'loss': ['hinge', 'squared_hinge'],\n",
    "        'fit_intercept': [True, False],\n",
    "        'max_iter': [1000, 5000, 10000]},\n",
    "        # K Neighbors\n",
    "        {'n_neighbors': [3, 5, 7, 9],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'algorithm': ['ball_tree', 'kd_tree', 'brute'],\n",
    "        'p': [1, 2]},\n",
    "        # Gradient Boosting\n",
    "        {'loss': ['deviance', 'exponential'],\n",
    "        'learning_rate': [0.001, 0.01, 0.1, 1],\n",
    "        'n_estimators': [10, 50, 100, 200],\n",
    "        'criterion': ['friedman_mse', 'squared_error', 'mae'],\n",
    "        'max_depth': [1, 3, 5, 7],\n",
    "        'max_features': ['auto', 'sqrt', 'log2']},\n",
    "        # Bagging\n",
    "        {'n_estimators': [5, 10, 50, 100, 200],\n",
    "        'max_features': [1, 4, 8, 16, 32],\n",
    "        'bootstrap': [True, False],\n",
    "        'bootstrap_features': [True, False],\n",
    "        'oob_score': [True, False]}        \n",
    "    ]\n",
    "\n",
    "    return search_spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the hyperparameters for each model\n",
    "\n",
    "if HYPER_SEARCH:\n",
    "    # Get the full dataset data\n",
    "    X, y, cols = load_raw_data(os.path.join(DATA_FOLDER, FULL_DATASET_NAME), FEATURE_TO_PREDICT)\n",
    "    # Perform the search\n",
    "    PARAMS = hyper_grid_search(X, y, get_binary_class_models(), get_bin_hyper_space(), 'accuracy')\n",
    "    # Save the found parameters\n",
    "    with open(os.path.join(RES_FOLDER, TYPE, 'params.pickle'), 'wb') as f:\n",
    "        pickle.dump(PARAMS, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "else:\n",
    "    with open(os.path.join(RES_FOLDER, TYPE, 'params.pickle'), 'rb') as f:\n",
    "        PARAMS = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the datasets and preparing them for training & testing\n",
    "DATASETS_NAMES, DATASETS, COLUMNS, FOLDS = load_processed_data(SEP_DATASETS_FOLDER, DATA_FOLDER, FEATURE_TO_PREDICT, NUM_FOLDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all the models\n",
    "trained_binary_class_models = train_models(get_binary_class_models, DATASETS, DATASETS_NAMES, FOLDS, PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all the models\n",
    "bin_class_results = evaluate_classification_models(trained_binary_class_models, DATASETS, DATASETS_NAMES, FOLDS, 'Binary', compute_metrics_bin)\n",
    "\n",
    "# Save the results to different files\n",
    "save_results(bin_class_results, RES_FOLDER, TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Classification\n",
    "In this section we introduce a new task: Multi Class classification.<br />\n",
    "We have different version of these multi class labels because there no a single multi label system present in the medical literature. Thus, we create 3 possible version of the labeling that will be examined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple function that generates a fresh batch of binary classification models ready to be trained\n",
    "\n",
    "def get_multi_class_models() -> list:\n",
    "    models =[(LogisticRegression(), 'Logistic Regression'), (GaussianNB(), 'Gaussian Naive Bayes'),\n",
    "              (LinearSVC(), 'Linear Support Vector Classifier'), (KNeighborsClassifier(), 'KNN Classifier'),\n",
    "              (GradientBoostingClassifier(), 'Boosted Trees'), (BaggingClassifier(), 'Bagged Trees')]\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple function that returns a list of dictionaries containing the hyperparameters state space of each model\n",
    "\n",
    "def get_multi_hyper_space():\n",
    "    # I use the same beacuse in my case there's no difference in the models used for binary and multiclass classification\n",
    "    return get_multi_hyper_space()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Classes\n",
    "- mmse >= 24 --> No Dementia\n",
    "- 18 <= mmse <= 23 --> Dementia\n",
    "- mmse <= 17 --> Severe Dementia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set some important variables first\n",
    "\n",
    "FEATURE_TO_PREDICT = 'multi_class3'\n",
    "NUM_FOLDS = 5\n",
    "TYPE = 'multi_classification3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the hyperparameters for each model\n",
    "\n",
    "if HYPER_SEARCH:\n",
    "    # Get the full dataset data\n",
    "    X, y, cols = load_raw_data(os.path.join(DATA_FOLDER, FULL_DATASET_NAME), FEATURE_TO_PREDICT)\n",
    "    # Perform the search\n",
    "    PARAMS = hyper_grid_search(X, y, get_binary_class_models(), get_bin_hyper_space(), 'accuracy')\n",
    "    # Save the found parameters\n",
    "    with open(os.path.join(RES_FOLDER, TYPE, 'params.pickle'), 'wb') as f:\n",
    "        pickle.dump(PARAMS, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "else:\n",
    "    with open(os.path.join(RES_FOLDER, TYPE, 'params.pickle'), 'rb') as f:\n",
    "        PARAMS = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the datasets and preparing them for training & testing\n",
    "DATASETS_NAMES, DATASETS, COLUMNS, FOLDS = load_processed_data(SEP_DATASETS_FOLDER, DATA_FOLDER, FEATURE_TO_PREDICT, NUM_FOLDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all the models\n",
    "trained_binary_class_models = train_models(get_multi_class_models, DATASETS, DATASETS_NAMES, FOLDS, PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all the models\n",
    "multi_class_results = evaluate_classification_models(trained_binary_class_models, DATASETS, DATASETS_NAMES, FOLDS, 'Multi CLass (3 classes)', compute_metrics_multi)\n",
    "\n",
    "# Save the results to different files\n",
    "save_results(multi_class_results, RES_FOLDER, TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Classes\n",
    "- mmse >= 26 --> No Dementia\n",
    "- 19 <= mmse <= 25 --> Mild Dementia\n",
    "- 10 <= mmse <= 18 --> Dementia\n",
    "- mmse <= 9 --> Severe Dementia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set some important variables first\n",
    "\n",
    "FEATURE_TO_PREDICT = 'multi_class4'\n",
    "NUM_FOLDS = 5\n",
    "TYPE = 'multi_classification4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the hyperparameters for each model\n",
    "\n",
    "if HYPER_SEARCH:\n",
    "    # Get the full dataset data\n",
    "    X, y, cols = load_raw_data(os.path.join(DATA_FOLDER, FULL_DATASET_NAME), FEATURE_TO_PREDICT)\n",
    "    # Perform the search\n",
    "    PARAMS = hyper_grid_search(X, y, get_binary_class_models(), get_bin_hyper_space(), 'accuracy')\n",
    "    # Save the found parameters\n",
    "    with open(os.path.join(RES_FOLDER, TYPE, 'params.pickle'), 'wb') as f:\n",
    "        pickle.dump(PARAMS, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "else:\n",
    "    with open(os.path.join(RES_FOLDER, TYPE, 'params.pickle'), 'rb') as f:\n",
    "        PARAMS = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the datasets and preparing them for training & testing\n",
    "DATASETS_NAMES, DATASETS, COLUMNS, FOLDS = load_processed_data(SEP_DATASETS_FOLDER, DATA_FOLDER, FEATURE_TO_PREDICT, NUM_FOLDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all the models\n",
    "trained_binary_class_models = train_models(get_multi_class_models, DATASETS, DATASETS_NAMES, FOLDS, PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all the models\n",
    "multi_class_results = evaluate_classification_models(trained_binary_class_models, DATASETS, DATASETS_NAMES, FOLDS, 'Multi CLass (4 classes)', compute_metrics_multi)\n",
    "\n",
    "# Save the results to different files\n",
    "save_results(multi_class_results, RES_FOLDER, TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 Classes\n",
    "- mmse == 30 --> No Dementia\n",
    "- 26 <= mmse <= 29 --> Possible Dementia\n",
    "- 19 <= mmse <= 25 --> Mild Dementia\n",
    "- 10 <= mmse <= 18 --> Dementia\n",
    "- mmse <= 9 --> Severe Dementia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set some important variables first\n",
    "\n",
    "FEATURE_TO_PREDICT = 'multi_class5'\n",
    "NUM_FOLDS = 5\n",
    "TYPE = 'multi_classification5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the hyperparameters for each model\n",
    "\n",
    "if HYPER_SEARCH:\n",
    "    # Get the full dataset data\n",
    "    X, y, cols = load_raw_data(os.path.join(DATA_FOLDER, FULL_DATASET_NAME), FEATURE_TO_PREDICT)\n",
    "    # Perform the search\n",
    "    PARAMS = hyper_grid_search(X, y, get_binary_class_models(), get_bin_hyper_space(), 'accuracy')\n",
    "    # Save the found parameters\n",
    "    with open(os.path.join(RES_FOLDER, TYPE, 'params.pickle'), 'wb') as f:\n",
    "        pickle.dump(PARAMS, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "else:\n",
    "    with open(os.path.join(RES_FOLDER, TYPE, 'params.pickle'), 'rb') as f:\n",
    "        PARAMS = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the datasets and preparing them for training & testing\n",
    "DATASETS_NAMES, DATASETS, COLUMNS, FOLDS = load_processed_data(SEP_DATASETS_FOLDER, DATA_FOLDER, FEATURE_TO_PREDICT, NUM_FOLDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all the models\n",
    "trained_binary_class_models = train_models(get_multi_class_models, DATASETS, DATASETS_NAMES, FOLDS, PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all the models\n",
    "multi_class_results = evaluate_classification_models(trained_binary_class_models, DATASETS, DATASETS_NAMES, FOLDS, 'Multi CLass (5 classes)', compute_metrics_multi)\n",
    "\n",
    "# Save the results to different files\n",
    "save_results(multi_class_results, RES_FOLDER, TYPE)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "64206bab08188e54a78281f612b2a1e836ab57c8a66e2c1578c55a21cced9d5c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('NN')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
